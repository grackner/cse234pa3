{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmRSlH1L5r-F"
      },
      "source": [
        "# CSE 234 Programming Assignment 3: Speculative Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDuj8yGG6EXg"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "52T8Gw-R5lup"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from typing import List, Tuple, Dict, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyZ4tAb6Gu2"
      },
      "source": [
        "## Speculative Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VIvmDG725x8H"
      },
      "outputs": [],
      "source": [
        "class SpeculativeDecoder:\n",
        "    def __init__(self, target_model_name: str, draft_model_name: str, device: str = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Initialize the speculative decoder with target and draft models.\n",
        "\n",
        "        Args:\n",
        "            target_model_name: HuggingFace model ID for the larger target model.\n",
        "            draft_model_name: HuggingFace model ID for the smaller draft model.\n",
        "            device: Device to run models on (\"cuda\" or \"cpu\").\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.target_model, self.target_tokenizer = self.initialize_target_model(target_model_name)\n",
        "        self.draft_model, self.draft_tokenizer = self.initialize_draft_model(draft_model_name)\n",
        "\n",
        "        # Ensure tokenizers are compatible\n",
        "        assert self.target_tokenizer.vocab == self.draft_tokenizer.vocab, \"Tokenizers must be compatible\"\n",
        "\n",
        "    def initialize_target_model(self, model_name: str):\n",
        "        \"\"\"Initialize the larger target model with caching enabled and proper pad token.\"\"\"\n",
        "        print(f\"Loading target model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Implement target model initialization\n",
        "        # 1. Set the pad token if it doesn't exist\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # 2. Load the model with appropriate settings for inference\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        # 3. Enable any optimizations that might help with performance\n",
        "        model = torch.compile(model)\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def initialize_draft_model(self, model_name: str):\n",
        "        \"\"\"\n",
        "        Initialize a smaller, faster draft model with proper pad token.\n",
        "        Uses lower precision and additional optimizations.\n",
        "        \"\"\"\n",
        "        print(f\"Loading draft model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Implement draft model initialization\n",
        "        # 1. Set the pad token if it doesn't exist\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "        # 2. Load the model with appropriate settings for inference\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
        "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "        model.eval()\n",
        "\n",
        "        # 3. Enable any optimizations that might help with performance\n",
        "        #model = torch.compile(model)\n",
        "        model.half()\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def generate_draft_tokens(self, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
        "                             num_speculative_tokens: int = 10) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate speculative tokens in one forward call using the draft model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs (tensor of shape [1, seq_len]).\n",
        "            attention_mask: Corresponding attention mask.\n",
        "            num_speculative_tokens: Number of tokens to speculate.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [1, num_speculative_tokens] containing the draft tokens.\n",
        "        \"\"\"\n",
        "        # TODO: Implement draft token generation\n",
        "        # 1. Use the draft model to generate tokens\n",
        "        with torch.inference_mode(): #torch.no_grad():\n",
        "            draft_tokens = self.draft_model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=num_speculative_tokens\n",
        "            )\n",
        "\n",
        "        # 2. Extract only the new tokens (not including the input)\n",
        "        #print('INPUT TOKENS', input_ids)\n",
        "        #print('DRAFT TOKENS', draft_tokens)\n",
        "        new_tokens = draft_tokens[:, input_ids.shape[1]:]\n",
        "\n",
        "        # 3. Return the newly generated tokens\n",
        "        return new_tokens\n",
        "\n",
        "    def verify_tokens_vectorized(self, input_ids: torch.Tensor, draft_tokens: torch.Tensor,\n",
        "                               attention_mask: torch.Tensor) -> Tuple[List[int], int]:\n",
        "        \"\"\"\n",
        "        Vectorized verification: verify all draft tokens in one forward pass using the target model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: The current input token IDs (shape [1, L]).\n",
        "            draft_tokens: Draft tokens from the draft model (shape [1, k]).\n",
        "            attention_mask: The current attention mask for input_ids.\n",
        "\n",
        "        Returns:\n",
        "            accepted_tokens: List of accepted token IDs.\n",
        "            accepted_position: Index of the first rejected token (if all accepted, equals draft_tokens.shape[1]).\n",
        "        \"\"\"\n",
        "        # TODO: Implement efficient verification of draft tokens\n",
        "        # 1. Run target model on input_ids concatenated with draft_tokens\n",
        "        extended_input = torch.cat([input_ids, draft_tokens], dim=1)\n",
        "        extended_mask = torch.cat([attention_mask, torch.ones_like(draft_tokens, device=self.device)], dim=1)\n",
        "\n",
        "        # 2. Extract the logits for positions where draft tokens would be predicted\n",
        "        with torch.inference_mode(): #torch.no_grad():\n",
        "            logits = self.target_model(extended_input, attention_mask=extended_mask).logits\n",
        "\n",
        "            # 3. Compare target model predictions with draft tokens\n",
        "            start_idx = input_ids.shape[1]-1\n",
        "            end_idx = start_idx + draft_tokens.shape[1]\n",
        "            target_predictions = logits[:, start_idx:end_idx, :]\n",
        "            matches = (target_predictions.argmax(dim=-1) == draft_tokens)\n",
        "\n",
        "\n",
        "        #print('target predictions', target_predictions.argmax(dim=-1))\n",
        "        #print('draft tokens', draft_tokens)\n",
        "        #print('First mismatch', first_mismatch_idx)\n",
        "        # 4. Determine how many consecutive tokens were accepted before first mismatch\n",
        "        accepted_position = torch.where(~matches)[1].min().item() if not matches.all() else draft_tokens.shape[1]\n",
        "        accepted_tokens = draft_tokens[:, :accepted_position].tolist() #[0]\n",
        "\n",
        "        return accepted_tokens, accepted_position\n",
        "\n",
        "    def speculative_decode(self, prompt: str, max_tokens: int = 100,\n",
        "                          num_speculative_tokens: int = 15) -> str: #15 for pythia, 9 for gpt2\n",
        "        \"\"\"\n",
        "        Main speculative decoding algorithm with vectorized verification.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text.\n",
        "            max_tokens: Maximum number of tokens to generate (excluding prompt).\n",
        "            num_speculative_tokens: Number of tokens to speculate per iteration.\n",
        "\n",
        "        Returns:\n",
        "            Generated text.\n",
        "        \"\"\"\n",
        "        # Tokenize prompt\n",
        "        inputs = self.target_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "        prompt_length = input_ids.shape[1]\n",
        "\n",
        "        # Initialize counters for performance tracking\n",
        "        total_tokens_generated = prompt_length\n",
        "        total_draft_tokens_proposed = 0\n",
        "        total_draft_tokens_accepted = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # TODO: Implement the core speculative decoding loop\n",
        "        # 1. Generate draft tokens using the draft model\n",
        "        while total_tokens_generated - prompt_length < max_tokens:\n",
        "            draft_tokens = self.generate_draft_tokens(input_ids, attention_mask, num_speculative_tokens)\n",
        "            total_draft_tokens_proposed += draft_tokens.shape[1]\n",
        "\n",
        "            # 2. Verify draft tokens using the target model\n",
        "            accepted_tokens, accepted_position = self.verify_tokens_vectorized(input_ids, draft_tokens, attention_mask)\n",
        "\n",
        "            # 3. Accept verified tokens and append to the sequence\n",
        "            total_draft_tokens_accepted += len(accepted_tokens[0])\n",
        "            input_ids = torch.cat([input_ids, draft_tokens[:, :accepted_position]], dim=1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones_like(draft_tokens[:, :accepted_position], device=self.device)], dim=1)\n",
        "            total_tokens_generated += accepted_position\n",
        "\n",
        "            # 4. For rejected tokens or if all tokens are accepted, generate a new token with the target model\n",
        "            if accepted_position < draft_tokens.shape[1]:\n",
        "                with torch.inference_mode(): #torch.no_grad():\n",
        "                    logits = self.target_model(input_ids, attention_mask=attention_mask).logits[:, -1, :]  # Get the logits for the last token\n",
        "                    next_token = logits.argmax(dim=-1, keepdim=True)  # Select most likely token\n",
        "\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "                attention_mask = torch.cat([attention_mask, torch.ones_like(next_token, device=self.device)], dim=1)\n",
        "                total_tokens_generated += 1\n",
        "\n",
        "            # 5. Stop when max_tokens is reached or an EOS token is generated\n",
        "            if self.target_tokenizer.eos_token_id in input_ids[0]:\n",
        "                break\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        elapsed_time = time.time() - start_time\n",
        "        acceptance_rate = total_draft_tokens_accepted / total_draft_tokens_proposed if total_draft_tokens_proposed > 0 else 0\n",
        "\n",
        "        print(f\"Generated {total_tokens_generated - prompt_length} tokens in {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Tokens per second: {(total_tokens_generated - prompt_length) / elapsed_time:.2f}\")\n",
        "        print(f\"Draft token acceptance rate: {acceptance_rate:.2%}\")\n",
        "\n",
        "        return self.target_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def benchmark(self, prompt: str, max_tokens: int = 100,\n",
        "                  num_runs: int = 3, compare_baseline: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Benchmark the speculative decoder against baseline decoding.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text.\n",
        "            max_tokens: Maximum number of tokens to generate.\n",
        "            num_runs: Number of benchmark runs.\n",
        "            compare_baseline: Whether to compare with baseline (non-speculative) decoding.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with benchmark results.\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"speculative\": {\"times\": [], \"tokens_per_second\": []},\n",
        "            \"baseline\": {\"times\": [], \"tokens_per_second\": []} if compare_baseline else None\n",
        "        }\n",
        "\n",
        "        # Benchmark speculative decoding.\n",
        "        for _ in range(num_runs):\n",
        "            start_time = time.time()\n",
        "            output = self.speculative_decode(prompt, max_tokens=max_tokens)\n",
        "            elapsed = time.time() - start_time\n",
        "            prompt_len = len(self.target_tokenizer(prompt)[\"input_ids\"])\n",
        "            output_tokens = len(self.target_tokenizer.encode(output)) - prompt_len\n",
        "            tps = output_tokens / elapsed\n",
        "            results[\"speculative\"][\"times\"].append(elapsed)\n",
        "            results[\"speculative\"][\"tokens_per_second\"].append(tps)\n",
        "\n",
        "        # Benchmark baseline decoding.\n",
        "        if compare_baseline:\n",
        "            for _ in range(num_runs):\n",
        "                inputs = self.target_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "                input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "                attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "                with torch.no_grad():\n",
        "                    output_ids = self.target_model.generate(\n",
        "                        input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_length=input_ids.shape[1] + max_tokens,\n",
        "                        do_sample=False,\n",
        "                        pad_token_id=self.target_tokenizer.pad_token_id\n",
        "                    )\n",
        "                elapsed = time.time() - start_time\n",
        "                output_tokens = output_ids.shape[1] - input_ids.shape[1]\n",
        "                tps = output_tokens / elapsed\n",
        "                results[\"baseline\"][\"times\"].append(elapsed)\n",
        "                results[\"baseline\"][\"tokens_per_second\"].append(tps)\n",
        "\n",
        "        for method in results.keys():\n",
        "            if results[method] is not None:\n",
        "                avg_time = sum(results[method][\"times\"]) / num_runs\n",
        "                avg_tps = sum(results[method][\"tokens_per_second\"]) / num_runs\n",
        "                results[method][\"avg_time\"] = avg_time\n",
        "                results[method][\"avg_tokens_per_second\"] = avg_tps\n",
        "\n",
        "        if compare_baseline:\n",
        "            speedup = results[\"baseline\"][\"avg_time\"] / results[\"speculative\"][\"avg_time\"]\n",
        "            results[\"speedup\"] = speedup\n",
        "            results[\"latency_reduction\"] = (1 - results[\"speculative\"][\"avg_time\"] / results[\"baseline\"][\"avg_time\"]) * 100\n",
        "            # print(f\"Speculative decoding speedup: {speedup:.2f}x\")\n",
        "            # print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNzh3cG-6KM0"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YyNXbA-26Cpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08d4eec-7df0-4af4-dace-f4d7b9251419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading target model: EleutherAI/pythia-1.4b-deduped\n",
            "Loading draft model: EleutherAI/pythia-160m-deduped\n",
            "\n",
            "Benchmarking Prompt 1:\n",
            "Prompt: The future of Artificial Intelligence is\n",
            "Generated 106 tokens in 1.85 seconds\n",
            "Tokens per second: 57.27\n",
            "Draft token acceptance rate: 87.50%\n",
            "Generated 106 tokens in 1.77 seconds\n",
            "Tokens per second: 59.81\n",
            "Draft token acceptance rate: 87.50%\n",
            "Generated 106 tokens in 1.80 seconds\n",
            "Tokens per second: 58.80\n",
            "Draft token acceptance rate: 87.50%\n",
            "Average speculative decoding time: 1.81 seconds\n",
            "Average speculative tokens per second: 58.58\n",
            "Average baseline decoding time: 2.71 seconds\n",
            "Average baseline tokens per second: 36.99\n",
            "Speedup: 1.50x\n",
            "Latency reduction: 33.27%\n",
            "\n",
            "Benchmarking Prompt 2:\n",
            "Prompt: Write a short story about a robot learning to feel emotions:\n",
            "Generated 114 tokens in 1.86 seconds\n",
            "Tokens per second: 61.24\n",
            "Draft token acceptance rate: 94.17%\n",
            "Generated 114 tokens in 1.84 seconds\n",
            "Tokens per second: 61.95\n",
            "Draft token acceptance rate: 94.17%\n",
            "Generated 114 tokens in 1.88 seconds\n",
            "Tokens per second: 60.71\n",
            "Draft token acceptance rate: 94.17%\n",
            "Average speculative decoding time: 1.86 seconds\n",
            "Average speculative tokens per second: 61.24\n",
            "Average baseline decoding time: 2.69 seconds\n",
            "Average baseline tokens per second: 37.47\n",
            "Speedup: 1.45x\n",
            "Latency reduction: 30.80%\n",
            "\n",
            "Benchmarking Prompt 3:\n",
            "Prompt: Write the lyrics to the song 'Happy Birthday'.\n",
            "Generated 108 tokens in 1.81 seconds\n",
            "Tokens per second: 59.57\n",
            "Draft token acceptance rate: 89.17%\n",
            "Generated 108 tokens in 1.87 seconds\n",
            "Tokens per second: 57.81\n",
            "Draft token acceptance rate: 89.17%\n",
            "Generated 108 tokens in 1.94 seconds\n",
            "Tokens per second: 55.78\n",
            "Draft token acceptance rate: 89.17%\n",
            "Average speculative decoding time: 1.87 seconds\n",
            "Average speculative tokens per second: 57.67\n",
            "Average baseline decoding time: 2.65 seconds\n",
            "Average baseline tokens per second: 37.88\n",
            "Speedup: 1.41x\n",
            "Latency reduction: 29.24%\n"
          ]
        }
      ],
      "source": [
        "target_model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Larger target model\n",
        "draft_model_name = \"EleutherAI/pythia-160m-deduped\"   # Smaller draft model\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O1EORd26MdC"
      },
      "source": [
        "## Bonus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y1sEo2706O29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f5edbe-4fb3-482e-c5e1-1f4e359a10f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading target model: openai-community/gpt2-large\n",
            "Loading draft model: openai-community/gpt2\n",
            "\n",
            "Benchmarking Prompt 1:\n",
            "Prompt: The future of Artificial Intelligence is\n",
            "Generated 105 tokens in 1.62 seconds\n",
            "Tokens per second: 64.69\n",
            "Draft token acceptance rate: 96.30%\n",
            "Generated 105 tokens in 1.72 seconds\n",
            "Tokens per second: 61.03\n",
            "Draft token acceptance rate: 96.30%\n",
            "Generated 105 tokens in 1.77 seconds\n",
            "Tokens per second: 59.48\n",
            "Draft token acceptance rate: 96.30%\n",
            "Average speculative decoding time: 1.71 seconds\n",
            "Average speculative tokens per second: 61.58\n",
            "Average baseline decoding time: 3.03 seconds\n",
            "Average baseline tokens per second: 33.02\n",
            "Speedup: 1.78x\n",
            "Latency reduction: 43.70%\n",
            "\n",
            "Benchmarking Prompt 2:\n",
            "Prompt: Write a short story about a robot learning to feel emotions:\n",
            "Generated 102 tokens in 3.13 seconds\n",
            "Tokens per second: 32.61\n",
            "Draft token acceptance rate: 56.17%\n",
            "Generated 102 tokens in 2.42 seconds\n",
            "Tokens per second: 42.22\n",
            "Draft token acceptance rate: 56.17%\n",
            "Generated 102 tokens in 2.34 seconds\n",
            "Tokens per second: 43.58\n",
            "Draft token acceptance rate: 56.17%\n",
            "Average speculative decoding time: 2.63 seconds\n",
            "Average speculative tokens per second: 39.45\n",
            "Average baseline decoding time: 3.11 seconds\n",
            "Average baseline tokens per second: 32.30\n",
            "Speedup: 1.18x\n",
            "Latency reduction: 15.47%\n",
            "\n",
            "Benchmarking Prompt 3:\n",
            "Prompt: Write the lyrics to the song 'Happy Birthday'.\n",
            "Generated 106 tokens in 1.78 seconds\n",
            "Tokens per second: 59.58\n",
            "Draft token acceptance rate: 80.95%\n",
            "Generated 106 tokens in 1.77 seconds\n",
            "Tokens per second: 59.91\n",
            "Draft token acceptance rate: 80.95%\n",
            "Generated 106 tokens in 1.76 seconds\n",
            "Tokens per second: 60.29\n",
            "Draft token acceptance rate: 80.95%\n",
            "Average speculative decoding time: 1.77 seconds\n",
            "Average speculative tokens per second: 59.88\n",
            "Average baseline decoding time: 3.12 seconds\n",
            "Average baseline tokens per second: 32.19\n",
            "Speedup: 1.76x\n",
            "Latency reduction: 43.25%\n"
          ]
        }
      ],
      "source": [
        "target_model_name = \"openai-community/gpt2-large\"  # Larger target model\n",
        "draft_model_name = \"openai-community/gpt2\"   # Smaller draft model\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report"
      ],
      "metadata": {
        "id": "KTQQ_kj3Y2dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My approach to implementing speculative decoding was to follow the skeleton code provided, and then focus on tuning the number of speculative tokens generated. As for setting up the speculative decoding implementation, it uses a two-stage approach to accelerate text generation. First, a smaller, faster draft model first predicts multiple tokens in a single forward pass. These draft tokens are then verified in parallel by a larger target model, which compares its own predicted probabilities to the draft outputs. If the draft tokens match the target model’s expectations, they are accepted, reducing the number of expensive target model forward passes. When a token is rejected, the target model directly generates the next token. This process should optimize efficiency with the smaller, quicker draft model, while still maintaining the quality of the target model, ultimately leading to faster decoding. As for my approach of tuning the number of speculative tokens generated, I focused on using as many speculative tokens as possible while still maintaining high draft token acceptance rate.\n",
        "\n",
        "\n",
        "One of the optimizations I implemented was using torch.compile() on the target model and torch.half() on the draft model. Adding compiler optimizations should optimize the computational graph to process draft tokens more efficiently, while reducing overhead in final token acceptance. Using half-precision on the draft model should reduce memory consumption and transfer, which should also lead to higher speedups. **However it is very important to note that since I am using torch.compile(), there needs to be one warmup run of the test before I am able to achieve good results.** This is because the computational graph needs to be traced initially so it knows how to optimize the kernel execution, so there needs to be one warmup run before recording results, so that once CUDA kernels are already compiled and cached, inference runs much faster.\n",
        "\n",
        "\n",
        "Another optimization I implemented was tuning the number of speculative tokens generated. I found that with the baseline pythia models, I could increase the number of speculative tokens to around 15-20 and achieve high acceptance. I found that when I increased the number of tokens to 20, I was getting higher speedup, but then the acceptance rate would drop slightly, so I ended up using 15 speculative tokens which worked well for these models. For the gpt-2 models (used in the bonus section), I found that it was quite easy to achieve significant speedup using anywhere from 10-20 speculative tokens generated, but the acceptance rate varied significantly. For example, when using 15-20 speculative tokens, the speedup could reach up to 2x for the first prompt and around 1.7x for the remaining two prompts, but the acceptance rate for the second two prompts would be ~45-55% (the first prompt always had >85% acceptance, regardless of the number of speculative tokens generated). However for these gpt-2 models, decreasing the speculative tokens to 9 led to the highest acceptance rates for all prompts. Still though, the gpt-2 models struggled with the second prompt, even after finding the optimal number of speculative tokens.\n",
        "\n",
        "\n",
        "To reiterate the results, I found that there was a tradeoff between achieving higher speedup or achieving higher draft token acceptance. Ultimately, I found that it was easier to increase the acceptance rate as opposed to the speedup, so this was the metric I chose to optimize. I ended up achieving 87.50%, 94.17%, and 89.17% draft token acceptance rates for the three respective prompts, with 1.50x, 1.45x, and 1.41x respective speedups with num_speculative_tokens=15. Also again I will mention that in order to achieve these results, the test needs to be run one time as a warm up so that the graph can be optimized properly.   \n",
        "\n",
        "\n",
        "One significant challenge I found was that in my initial implementation, I was only able to achieve 1/(# speculative tokens generated)% acceptance. For example, with the baseline of 15 speculative tokens generated, I was consistently only getting 1/15 = 6.67% draft token acceptance. No matter what optimizations I tried, I wasn’t able to change this acceptance rate. Then I started printing out what were the actual predicted tokens and what were the draft tokens, and I saw that the predicted tokens were shifted one from the draft tokens, which is why I was never able to get more than 1 accepted draft token. Thus, I decremented the predicted tokens by 1 (in the verify_tokens_vectorized() function: start_idx = input_ids.shape[1]-1), and this solved my problem. Now I was properly comparing predicted tokens with draft tokens, and my acceptance rate shot up to 80+%.\n"
      ],
      "metadata": {
        "id": "2FGeOY_rY5BB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
