The expert parallelized MoE performs better than the tensor parallelized MoE in our tests there is less communication overhead in expert parallelism. We also implemented an efficient version of expert parallelism by utilizing tensor operations in the function. The experts only have to communicate with each other which is faster than combining parts of experts. The expert parallelized MoE is also utilizing vectorized operations that are more efficient. Expert parallelism MoE completed a forward pass in an average of 0.0723 ms whereas the tensor parallelized MoE took 0.097 ms on average. The expert parallelized MoE also used less memory and had higher throughput. For throughput, the expert parallelized MoE completed around 110.61 samples / ms and tensor parallelized MoE completed around 82.17 samples per ms. Another reason why expert parallelism works better for our MoE than tensor parallelism is because our MoE is pretty sparse and expert parallelism is more efficient in that use case than tensor parallelism. There is less communication required. If we had a dense MoE with a lot of parameters or experts then tensor parallelism may be better in that use case. 